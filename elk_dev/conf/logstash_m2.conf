input {
	tcp {
		port => 5200
	}
#    kafka {
#        auto_offset_reset => "earliest"
#        group_id => "elk"
#        bootstrap_servers => "192.168.1.206:9092"
#        topics => [ "log" ]
#    }
}
filter {
  mutate {
     split => { "message" => "||" }
     add_field => { "requestTime" => "%{message[0]}" }
     add_field => { "responseTime" => "%{message[1]}" }
     add_field => { "hostIp" => "%{message[2]}" }
     add_field => { "uniqueKey" => "%{message[3]}" }
     add_field => { "errorMessage" => "%{message[4]}" }
     add_field => { "latency" => "%{message[5]}" }
     add_field => { "postAction" => "%{message[6]}" }
     add_field => { "customerId" => "%{message[7]}" }
     add_field => { "applicationCode" => "%{message[8]}" }
     add_field => { "requestNum" => "%{message[9]}" }
     add_field => { "branchCode" => "%{message[10]}" }
     add_field => { "customerAccount" => "%{message[11]}" }
     add_field => { "clerkId" => "%{message[12]}" }
     add_field => { "employeeId" => "%{message[13]}" }
     add_field => { "transactionCode" => "%{message[14]}" }
     add_field => { "clientIp" => "%{message[15]}" }
     add_field => { "gatewayCode" => "%{message[16]}" }
     add_field => { "responseCode" => "%{message[17]}" }
     remove_field => [ "message" ]
  }
}
## Add your filters / logstash plugins configuration here

output {
	elasticsearch {
		hosts => "192.168.1.206:9200"
		index => "dev_log_1-%{+YYYY.MM.dd}"
		user => elastic
		password => changeme
	}
#	stdout { 
#		codec => rubydebug
# 	} 
}
